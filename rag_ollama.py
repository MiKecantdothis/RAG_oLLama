import streamlit as st
import PyPDF2
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import ollama
import pickle
import os
from typing import List, Dict
import re

# Initialize session state
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'document_chunks' not in st.session_state:
    st.session_state.document_chunks = []
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []


class PDFChatbot:
    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.vector_store = None
        self.document_chunks = []

    def extract_text_from_pdf(self, pdf_file) -> str:
        """Extract text from uploaded PDF file"""
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text

    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if len(chunk.strip()) > 0:
                chunks.append(chunk.strip())

        return chunks

    def create_vector_store(self, chunks: List[str]):
        """Create FAISS vector store from text chunks"""
        embeddings = self.embedding_model.encode(chunks)

        # Create FAISS index
        dimension = embeddings.shape[1]
        self.vector_store = faiss.IndexFlatIP(dimension)

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)
        self.vector_store.add(embeddings.astype('float32'))
        self.document_chunks = chunks

    def search_similar_chunks(self, query: str, k: int = 3) -> List[str]:
        """Search for similar chunks in the vector store"""
        if self.vector_store is None:
            return []

        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)

        scores, indices = self.vector_store.search(query_embedding.astype('float32'), k)

        relevant_chunks = []
        for idx in indices[0]:
            if idx < len(self.document_chunks):
                relevant_chunks.append(self.document_chunks[idx])

        return relevant_chunks

    def generate_response(self, query: str, context: List[str]) -> str:
        """Generate response using Llama model with context"""
        context_text = "\n".join(context)

        prompt = f"""Based on the following context from the document, please answer the question. If the answer cannot be found in the context, please say so.

Context:
{context_text}

Question: {query}

Answer:"""

        try:
            response = ollama.chat(
                model='llama3.3',  # You can change this to your preferred Llama model
                messages=[
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ]
            )
            return response['message']['content']
        except Exception as e:
            return f"Error generating response: {str(e)}. Make sure Ollama is running and the model is installed."

def main():
    st.title("üìö RAG PDF Chatbot with Llama")
    st.markdown("Upload a PDF document and chat with it using local Llama model!")

    # Initialize chatbot
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = PDFChatbot()

    # Sidebar for PDF upload and processing
    with st.sidebar:
        st.header("üìÑ Document Upload")
        uploaded_file = st.file_uploader(
            "Choose a PDF file",
            type="pdf",
            help="Upload a PDF document to chat with"
        )

        if uploaded_file is not None:
            if st.button("Process Document", type="primary"):
                with st.spinner("Processing PDF..."):
                    # Extract text
                    text = st.session_state.chatbot.extract_text_from_pdf(uploaded_file)

                    # Chunk text
                    chunks = st.session_state.chatbot.chunk_text(text)

                    # Create vector store
                    st.session_state.chatbot.create_vector_store(chunks)

                    # Store in session state
                    st.session_state.vector_store = st.session_state.chatbot.vector_store
                    st.session_state.document_chunks = st.session_state.chatbot.document_chunks

                    st.success(f"‚úÖ Document processed! Created {len(chunks)} chunks.")

        # Display document info
        if st.session_state.vector_store is not None:
            st.success("üìä Document Ready")
            st.info(f"Chunks: {len(st.session_state.document_chunks)}")

        st.markdown("---")
        st.markdown("### üõ†Ô∏è Requirements")
        st.markdown("""
        Make sure you have:
        1. **Ollama** installed and running
        2. **Llama model** downloaded (e.g., `ollama pull llama3.2`)
        3. All required Python packages installed
        """)

        if st.button("Clear Chat History"):
            st.session_state.chat_history = []
            st.rerun()

    # Main chat interface
    if st.session_state.vector_store is None:
        st.info("üëÜ Please upload and process a PDF document to start chatting!")
        return

    # Display chat history
    st.subheader("üí¨ Chat History")
    chat_container = st.container()

    with chat_container:
        for i, (query, response) in enumerate(st.session_state.chat_history):
            with st.chat_message("user"):
                st.write(query)
            with st.chat_message("assistant"):
                st.write(response)

    # Chat input
    query = st.chat_input("Ask a question about your document...")

    if query:
        # Add user message to chat history
        with st.chat_message("user"):
            st.write(query)

        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                # Update chatbot's vector store and chunks
                st.session_state.chatbot.vector_store = st.session_state.vector_store
                st.session_state.chatbot.document_chunks = st.session_state.document_chunks

                # Search for relevant chunks
                relevant_chunks = st.session_state.chatbot.search_similar_chunks(query)

                # Generate response
                response = st.session_state.chatbot.generate_response(query, relevant_chunks)

                st.write(response)

                # Add to chat history
                st.session_state.chat_history.append((query, response))

        st.rerun()

if __name__ == "__main__":
    # Check for required packages
    try:
        main()
    except ImportError as e:
        st.error(f"""
        Missing required package: {e}

        Please install the required packages:
        ```bash
        pip install streamlit PyPDF2 faiss-cpu sentence-transformers ollama numpy
        ```
        """)

